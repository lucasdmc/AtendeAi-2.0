# ğŸ’¬ ENTREGÃVEL 3: CONVERSATION SERVICE - ATENDEAI 2.0

---

## ğŸ¯ **OBJETIVO**

Implementar o **Conversation Service** completo com sistema de memÃ³ria conversacional avanÃ§ado, orquestraÃ§Ã£o de LLMs, e gestÃ£o inteligente de contexto para conversas naturais e personalizadas por clÃ­nica.

---

## ğŸ“‹ **ESCOPO DO ENTREGÃVEL**

### **Sistema de MemÃ³ria Conversacional**
- [ ] MemÃ³ria de curto prazo (sessÃ£o atual)
- [ ] MemÃ³ria de longo prazo (histÃ³rico de conversas)
- [ ] Sistema de cache inteligente
- [ ] Limpeza automÃ¡tica de memÃ³ria expirada

### **OrquestraÃ§Ã£o de LLMs**
- [ ] IntegraÃ§Ã£o com mÃºltiplos provedores (OpenAI, Anthropic, Google)
- [ ] Roteamento inteligente baseado em contexto
- [ ] Fallback automÃ¡tico entre modelos
- [ ] Balanceamento de carga entre provedores

### **GestÃ£o de Contexto**
- [ ] Contexto dinÃ¢mico por conversa
- [ ] PersonalizaÃ§Ã£o por clÃ­nica
- [ ] HistÃ³rico de intenÃ§Ãµes
- [ ] Rastreamento de estado da conversa

### **Sistema de IntenÃ§Ãµes**
- [ ] DetecÃ§Ã£o automÃ¡tica de intenÃ§Ãµes
- [ ] ClassificaÃ§Ã£o de confianÃ§a
- [ ] Fallbacks configurÃ¡veis
- [ ] Aprendizado contÃ­nuo

---

## ğŸ—ï¸ **ARQUITETURA DO SERVIÃ‡O**

### **Componentes Principais**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONVERSATION SERVICE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conversation   â”‚  Message        â”‚  Memory         â”‚  LLM    â”‚
â”‚   Manager       â”‚  Processor      â”‚   Manager       â”‚Orchestr.â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Intent         â”‚  Context        â”‚  Cache          â”‚  Audit  â”‚
â”‚  Detector       â”‚  Manager        â”‚  Service        â”‚ Service â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  INFRASTRUCTURE LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   PostgreSQL    â”‚     Redis       â”‚   LLM APIs      â”‚ Logging â”‚
â”‚   (Database)    â”‚    (Cache)      â”‚   (External)    â”‚ (Winston)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Endpoints da API**
```
# GestÃ£o de Conversas
POST   /api/conversations              # Iniciar nova conversa
GET    /api/conversations/:id          # Buscar conversa
PUT    /api/conversations/:id          # Atualizar conversa
DELETE /api/conversations/:id          # Encerrar conversa
GET    /api/conversations/:id/status   # Status da conversa

# Mensagens
POST   /api/conversations/:id/messages # Enviar mensagem
GET    /api/conversations/:id/messages # HistÃ³rico de mensagens
PUT    /api/messages/:id               # Atualizar mensagem
DELETE /api/messages/:id               # Deletar mensagem

# MemÃ³ria Conversacional
GET    /api/conversations/:id/memory   # Buscar memÃ³ria
PUT    /api/conversations/:id/memory   # Atualizar memÃ³ria
DELETE /api/conversations/:id/memory   # Limpar memÃ³ria
POST   /api/conversations/:id/memory/clear # Limpeza automÃ¡tica

# Contexto
GET    /api/conversations/:id/context  # Buscar contexto atual
PUT    /api/conversations/:id/context  # Atualizar contexto
POST   /api/conversations/:id/context/reset # Resetar contexto

# IntenÃ§Ãµes
GET    /api/conversations/:id/intents  # HistÃ³rico de intenÃ§Ãµes
POST   /api/conversations/:id/intents/analyze # Analisar mensagem
GET    /api/intents/statistics         # EstatÃ­sticas de intenÃ§Ãµes

# LLM
POST   /api/llm/generate              # Gerar resposta
POST   /api/llm/analyze               # Analisar texto
GET    /api/llm/providers             # Listar provedores
GET    /api/llm/status                # Status dos provedores
```

---

## ğŸ—„ï¸ **MODELOS DE DADOS**

### **Tabela: `conversations` (Schema: `conversation`)**
```sql
CREATE TABLE conversation.conversations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    clinic_id UUID NOT NULL REFERENCES atendeai.clinics(id) ON DELETE CASCADE,
    whatsapp_number VARCHAR(20) NOT NULL,
    user_name VARCHAR(255),
    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'paused', 'ended')),
    context JSONB DEFAULT '{}',
    last_message_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Campos JSONB:**
- `context`: Contexto da conversa (intenÃ§Ãµes, dados coletados, estado atual)

### **Tabela: `messages` (Schema: `conversation`)**
```sql
CREATE TABLE conversation.messages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES conversation.conversations(id) ON DELETE CASCADE,
    clinic_id UUID NOT NULL REFERENCES atendeai.clinics(id) ON DELETE CASCADE,
    message_type VARCHAR(20) NOT NULL CHECK (message_type IN ('incoming', 'outgoing')),
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    processed BOOLEAN DEFAULT false,
    intent_detected VARCHAR(100),
    confidence_score DECIMAL(3,2),
    llm_provider VARCHAR(50),
    llm_model VARCHAR(50),
    response_time_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Campos JSONB:**
- `metadata`: Metadados da mensagem (tipo de mÃ­dia, status, tokens, etc.)

### **Tabela: `conversation_memory` (Schema: `conversation`)**
```sql
CREATE TABLE conversation.conversation_memory (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES conversation.conversations(id) ON DELETE CASCADE,
    clinic_id UUID NOT NULL REFERENCES atendeai.clinics(id) ON DELETE CASCADE,
    memory_key VARCHAR(100) NOT NULL,
    memory_value JSONB NOT NULL,
    memory_type VARCHAR(20) DEFAULT 'session' CHECK (memory_type IN ('session', 'long_term', 'context')),
    expires_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    UNIQUE(conversation_id, memory_key)
);
```

**Campos JSONB:**
- `memory_value`: Valor da memÃ³ria (pode ser qualquer tipo de dado)

### **Tabela: `intent_history` (Schema: `conversation`)**
```sql
CREATE TABLE conversation.intent_history (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES conversation.conversations(id) ON DELETE CASCADE,
    clinic_id UUID NOT NULL REFERENCES atendeai.clinics(id) ON DELETE CASCADE,
    message_id UUID REFERENCES conversation.messages(id),
    intent_name VARCHAR(100) NOT NULL,
    confidence_score DECIMAL(3,2) NOT NULL,
    entities JSONB DEFAULT '{}',
    fallback_used BOOLEAN DEFAULT false,
    llm_provider VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Campos JSONB:**
- `entities`: Entidades extraÃ­das da mensagem

### **Tabela: `llm_providers` (Schema: `conversation`)**
```sql
CREATE TABLE conversation.llm_providers (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    clinic_id UUID NOT NULL REFERENCES atendeai.clinics(id) ON DELETE CASCADE,
    provider_name VARCHAR(50) NOT NULL,
    api_key VARCHAR(500),
    api_endpoint VARCHAR(500),
    models JSONB DEFAULT '[]',
    is_active BOOLEAN DEFAULT true,
    priority INTEGER DEFAULT 1,
    rate_limit_per_minute INTEGER DEFAULT 60,
    cost_per_token DECIMAL(10,6),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    UNIQUE(clinic_id, provider_name)
);
```

**Campos JSONB:**
- `models`: Lista de modelos disponÃ­veis para este provedor

---

## ğŸ§  **SISTEMA DE MEMÃ“RIA CONVERSACIONAL**

### **Tipos de MemÃ³ria**

#### **1. MemÃ³ria de SessÃ£o (Session Memory)**
- **DuraÃ§Ã£o**: Durante a conversa atual
- **Escopo**: Conversa especÃ­fica
- **ConteÃºdo**: Contexto imediato, dados coletados
- **Exemplo**: Nome do paciente, sintomas, preferÃªncias de horÃ¡rio

#### **2. MemÃ³ria de Longo Prazo (Long-term Memory)**
- **DuraÃ§Ã£o**: Indefinida (atÃ© limpeza manual)
- **Escopo**: UsuÃ¡rio especÃ­fico
- **ConteÃºdo**: PreferÃªncias, histÃ³rico mÃ©dico, padrÃµes de comportamento
- **Exemplo**: Alergias conhecidas, horÃ¡rios preferidos, histÃ³rico de consultas

#### **3. MemÃ³ria de Contexto (Context Memory)**
- **DuraÃ§Ã£o**: ConfigurÃ¡vel por clÃ­nica
- **Escopo**: ClÃ­nica especÃ­fica
- **ConteÃºdo**: ConfiguraÃ§Ãµes da IA, respostas padrÃ£o, fluxos de conversa
- **Exemplo**: Personalidade da IA, polÃ­ticas de agendamento, horÃ¡rios da clÃ­nica

### **Estrutura da MemÃ³ria**
```json
{
  "memory_key": "patient_preferences",
  "memory_value": {
    "preferred_time": "morning",
    "avoid_days": ["monday", "friday"],
    "special_requests": "acessibilidade para cadeirantes",
    "last_consultation": "2024-01-10",
    "favorite_doctor": "Dr. Silva"
  },
  "memory_type": "long_term",
  "expires_at": null,
  "metadata": {
    "source": "user_input",
    "confidence": 0.95,
    "last_updated": "2024-01-15T10:00:00Z"
  }
}
```

### **Sistema de Cache Inteligente**
```javascript
class MemoryCacheService {
  constructor() {
    this.sessionCache = new Map(); // MemÃ³ria de sessÃ£o
    this.longTermCache = new Map(); // MemÃ³ria de longo prazo
    this.contextCache = new Map(); // MemÃ³ria de contexto
  }

  // Buscar memÃ³ria com fallback
  async getMemory(conversationId, key, type = 'session') {
    // 1. Tentar cache local
    const cacheKey = `${conversationId}:${key}`;
    let memory = this.getFromLocalCache(cacheKey, type);
    
    if (memory) return memory;

    // 2. Buscar no banco
    memory = await this.getFromDatabase(conversationId, key, type);
    
    if (memory) {
      // 3. Atualizar cache local
      this.updateLocalCache(cacheKey, memory, type);
      return memory;
    }

    // 4. Fallback para contexto da clÃ­nica
    return await this.getClinicContext(conversationId, key);
  }

  // Atualizar memÃ³ria
  async updateMemory(conversationId, key, value, type = 'session') {
    // 1. Atualizar cache local
    const cacheKey = `${conversationId}:${key}`;
    this.updateLocalCache(cacheKey, value, type);

    // 2. Persistir no banco
    await this.persistToDatabase(conversationId, key, value, type);

    // 3. Notificar outros serviÃ§os se necessÃ¡rio
    await this.notifyMemoryUpdate(conversationId, key, value);
  }

  // Limpeza automÃ¡tica
  async cleanupExpiredMemory() {
    const now = new Date();
    
    // Limpar memÃ³rias expiradas
    for (const [key, memory] of this.sessionCache.entries()) {
      if (memory.expires_at && memory.expires_at < now) {
        this.sessionCache.delete(key);
      }
    }

    // Limpar memÃ³rias de contexto expiradas
    for (const [key, memory] of this.contextCache.entries()) {
      if (memory.expires_at && memory.expires_at < now) {
        this.contextCache.delete(key);
      }
    }
  }
}
```

---

## ğŸ¤– **ORQUESTRAÃ‡ÃƒO DE LLMS**

### **Provedores Suportados**

#### **1. OpenAI (GPT-4, GPT-3.5-turbo)**
```javascript
class OpenAIProvider {
  constructor(apiKey, models = ['gpt-4', 'gpt-3.5-turbo']) {
    this.apiKey = apiKey;
    this.models = models;
    this.baseURL = 'https://api.openai.com/v1';
  }

  async generateResponse(prompt, context, model = 'gpt-4') {
    try {
      const response = await openai.chat.completions.create({
        model: model,
        messages: [
          { role: 'system', content: this.buildSystemPrompt(context) },
          { role: 'user', content: prompt }
        ],
        max_tokens: 1000,
        temperature: 0.7,
        presence_penalty: 0.1,
        frequency_penalty: 0.1
      });

      return {
        content: response.choices[0].message.content,
        provider: 'openai',
        model: model,
        tokens_used: response.usage.total_tokens,
        cost: this.calculateCost(response.usage.total_tokens, model)
      };
    } catch (error) {
      throw new Error(`OpenAI Error: ${error.message}`);
    }
  }

  buildSystemPrompt(context) {
    return `VocÃª Ã© um assistente virtual mÃ©dico chamado ${context.ai_personality.name}.
    
    Personalidade: ${context.ai_personality.tone}
    NÃ­vel de formalidade: ${context.ai_personality.formality_level}
    Idiomas: ${context.ai_personality.languages.join(', ')}
    
    HorÃ¡rios da clÃ­nica: ${JSON.stringify(context.working_hours)}
    PolÃ­ticas de agendamento: ${JSON.stringify(context.scheduling_policies)}
    
    Seja ${context.ai_personality.response_style} e ${context.ai_behavior.proactivity ? 'proativo' : 'reativo'}.
    
    Respostas devem ter no mÃ¡ximo ${context.ai_behavior.max_response_length} caracteres.
    
    Se nÃ£o souber algo, use a estratÃ©gia: ${context.ai_behavior.fallback_strategy}`;
  }
}
```

#### **2. Anthropic (Claude)**
```javascript
class AnthropicProvider {
  constructor(apiKey, models = ['claude-3-sonnet-20240229', 'claude-3-haiku-20240307']) {
    this.apiKey = apiKey;
    this.models = models;
    this.baseURL = 'https://api.anthropic.com/v1';
  }

  async generateResponse(prompt, context, model = 'claude-3-sonnet-20240229') {
    try {
      const response = await anthropic.messages.create({
        model: model,
        max_tokens: 1000,
        temperature: 0.7,
        system: this.buildSystemPrompt(context),
        messages: [{ role: 'user', content: prompt }]
      });

      return {
        content: response.content[0].text,
        provider: 'anthropic',
        model: model,
        tokens_used: response.usage.input_tokens + response.usage.output_tokens,
        cost: this.calculateCost(response.usage.input_tokens + response.usage.output_tokens, model)
      };
    } catch (error) {
      throw new Error(`Anthropic Error: ${error.message}`);
    }
  }
}
```

#### **3. Google (Gemini)**
```javascript
class GoogleProvider {
  constructor(apiKey, models = ['gemini-pro', 'gemini-pro-vision']) {
    this.apiKey = apiKey;
    this.models = models;
    this.baseURL = 'https://generativelanguage.googleapis.com/v1beta';
  }

  async generateResponse(prompt, context, model = 'gemini-pro') {
    try {
      const response = await google.generativeAI.generateContent({
        model: model,
        contents: [
          { role: 'user', parts: [{ text: this.buildSystemPrompt(context) + '\n\n' + prompt }] }
        ],
        generationConfig: {
          maxOutputTokens: 1000,
          temperature: 0.7,
          topP: 0.8,
          topK: 40
        }
      });

      return {
        content: response.response.text(),
        provider: 'google',
        model: model,
        tokens_used: response.response.usageMetadata.totalTokenCount,
        cost: this.calculateCost(response.response.usageMetadata.totalTokenCount, model)
      };
    } catch (error) {
      throw new Error(`Google Error: ${error.message}`);
    }
  }
}
```

### **Orquestrador Inteligente**
```javascript
class LLMOrchestrator {
  constructor() {
    this.providers = new Map();
    this.fallbackChain = [];
    this.healthChecks = new Map();
  }

  // Registrar provedor
  registerProvider(provider) {
    this.providers.set(provider.name, provider);
    this.fallbackChain.push(provider.name);
    this.healthChecks.set(provider.name, { status: 'healthy', lastCheck: Date.now() });
  }

  // Gerar resposta com fallback
  async generateResponse(prompt, context, preferredProvider = null) {
    let lastError = null;
    
    // Tentar provedor preferido primeiro
    if (preferredProvider && this.providers.has(preferredProvider)) {
      try {
        return await this.providers.get(preferredProvider).generateResponse(prompt, context);
      } catch (error) {
        lastError = error;
        this.updateProviderHealth(preferredProvider, 'unhealthy');
      }
    }

    // Tentar outros provedores na ordem de fallback
    for (const providerName of this.fallbackChain) {
      if (providerName === preferredProvider) continue;
      
      try {
        const provider = this.providers.get(providerName);
        if (provider && this.isProviderHealthy(providerName)) {
          return await provider.generateResponse(prompt, context);
        }
      } catch (error) {
        lastError = error;
        this.updateProviderHealth(providerName, 'unhealthy');
      }
    }

    // Todos os provedores falharam
    throw new Error(`All LLM providers failed. Last error: ${lastError.message}`);
  }

  // Verificar saÃºde dos provedores
  async checkProviderHealth() {
    for (const [providerName, provider] of this.providers.entries()) {
      try {
        // Teste simples de conectividade
        await provider.generateResponse('Test', { ai_personality: { name: 'Test' } });
        this.updateProviderHealth(providerName, 'healthy');
      } catch (error) {
        this.updateProviderHealth(providerName, 'unhealthy');
      }
    }
  }

  // Balanceamento de carga
  getOptimalProvider(context) {
    const healthyProviders = Array.from(this.healthChecks.entries())
      .filter(([_, health]) => health.status === 'healthy')
      .map(([name, _]) => name);

    if (healthyProviders.length === 0) {
      throw new Error('No healthy LLM providers available');
    }

    // LÃ³gica de seleÃ§Ã£o baseada em contexto
    if (context.requires_vision && healthyProviders.includes('gemini-pro-vision')) {
      return 'gemini-pro-vision';
    }

    if (context.requires_long_context && healthyProviders.includes('claude-3-sonnet')) {
      return 'claude-3-sonnet';
    }

    // Retornar o primeiro saudÃ¡vel
    return healthyProviders[0];
  }
}
```

---

## ğŸ¯ **SISTEMA DE DETECÃ‡ÃƒO DE INTENÃ‡Ã•ES**

### **ClassificaÃ§Ã£o de IntenÃ§Ãµes**
```javascript
class IntentDetector {
  constructor() {
    this.intentPatterns = new Map();
    this.confidenceThreshold = 0.7;
    this.loadIntentPatterns();
  }

  // Detectar intenÃ§Ã£o de uma mensagem
  async detectIntent(message, context) {
    // 1. AnÃ¡lise baseada em padrÃµes
    const patternMatch = this.analyzePatterns(message);
    
    // 2. AnÃ¡lise com LLM
    const llmAnalysis = await this.analyzeWithLLM(message, context);
    
    // 3. CombinaÃ§Ã£o de resultados
    const combinedResult = this.combineAnalyses(patternMatch, llmAnalysis);
    
    // 4. ValidaÃ§Ã£o de confianÃ§a
    if (combinedResult.confidence >= this.confidenceThreshold) {
      return combinedResult;
    }

    // 5. Fallback para intenÃ§Ã£o genÃ©rica
    return {
      intent: 'information_request',
      confidence: 0.5,
      entities: {},
      fallback_used: true
    };
  }

  // AnÃ¡lise baseada em padrÃµes
  analyzePatterns(message) {
    const lowerMessage = message.toLowerCase();
    
    // PadrÃµes para agendamento
    if (lowerMessage.includes('marcar') || lowerMessage.includes('agendar') || 
        lowerMessage.includes('consulta') || lowerMessage.includes('horÃ¡rio')) {
      return {
        intent: 'appointment_booking',
        confidence: 0.8,
        entities: this.extractEntities(message)
      };
    }

    // PadrÃµes para reagendamento
    if (lowerMessage.includes('reagendar') || lowerMessage.includes('mudar') || 
        lowerMessage.includes('alterar') || lowerMessage.includes('trocar')) {
      return {
        intent: 'appointment_reschedule',
        confidence: 0.8,
        entities: this.extractEntities(message)
      };
    }

    // PadrÃµes para cancelamento
    if (lowerMessage.includes('cancelar') || lowerMessage.includes('desmarcar') || 
        lowerMessage.includes('nÃ£o quero') || lowerMessage.includes('desistir')) {
      return {
        intent: 'appointment_cancellation',
        confidence: 0.8,
        entities: this.extractEntities(message)
      };
    }

    // PadrÃµes para informaÃ§Ãµes
    if (lowerMessage.includes('horÃ¡rio') || lowerMessage.includes('funciona') || 
        lowerMessage.includes('aberto') || lowerMessage.includes('fechado')) {
      return {
        intent: 'information_request',
        confidence: 0.7,
        entities: this.extractEntities(message)
      };
    }

    return {
      intent: 'unknown',
      confidence: 0.3,
      entities: {}
    };
  }

  // AnÃ¡lise com LLM
  async analyzeWithLLM(message, context) {
    try {
      const prompt = `Analise a seguinte mensagem e identifique a intenÃ§Ã£o do usuÃ¡rio:

Mensagem: "${message}"

Contexto da clÃ­nica:
- Personalidade da IA: ${context.ai_personality.name}
- HorÃ¡rios: ${JSON.stringify(context.working_hours)}
- ServiÃ§os: ${JSON.stringify(context.services)}

Classifique a intenÃ§Ã£o em uma das seguintes categorias:
1. appointment_booking - UsuÃ¡rio quer marcar uma consulta
2. appointment_reschedule - UsuÃ¡rio quer reagendar uma consulta
3. appointment_cancellation - UsuÃ¡rio quer cancelar uma consulta
4. information_request - UsuÃ¡rio quer informaÃ§Ãµes sobre a clÃ­nica
5. complaint - UsuÃ¡rio tem uma reclamaÃ§Ã£o
6. feedback - UsuÃ¡rio quer dar feedback
7. other - Outra intenÃ§Ã£o nÃ£o classificada

Responda apenas com um JSON no formato:
{
  "intent": "nome_da_intencao",
  "confidence": 0.95,
  "entities": {
    "date": "2024-01-20",
    "time": "14:00",
    "service": "consulta mÃ©dica"
  },
  "reasoning": "Breve explicaÃ§Ã£o da classificaÃ§Ã£o"
}`;

      const response = await this.llmOrchestrator.generateResponse(prompt, context);
      return JSON.parse(response.content);
    } catch (error) {
      return {
        intent: 'unknown',
        confidence: 0.3,
        entities: {},
        reasoning: 'LLM analysis failed'
      };
    }
  }

  // Extrair entidades da mensagem
  extractEntities(message) {
    const entities = {};
    
    // Extrair datas
    const datePattern = /(\d{1,2}\/\d{1,2}\/\d{4})|(\d{1,2}-\d{1,2}-\d{4})|(hoje|amanhÃ£|depois de amanhÃ£)/gi;
    const dateMatches = message.match(datePattern);
    if (dateMatches) {
      entities.dates = dateMatches;
    }

    // Extrair horÃ¡rios
    const timePattern = /(\d{1,2}:\d{2})|(\d{1,2}h)|(manhÃ£|tarde|noite)/gi;
    const timeMatches = message.match(timePattern);
    if (timeMatches) {
      entities.times = timeMatches;
    }

    // Extrair serviÃ§os
    const servicePattern = /(consulta|exame|procedimento|cirurgia)/gi;
    const serviceMatches = message.match(servicePattern);
    if (serviceMatches) {
      entities.services = serviceMatches;
    }

    return entities;
  }

  // Combinar anÃ¡lises
  combineAnalyses(patternAnalysis, llmAnalysis) {
    // Peso para cada tipo de anÃ¡lise
    const patternWeight = 0.4;
    const llmWeight = 0.6;

    // Se as intenÃ§Ãµes sÃ£o iguais, aumentar confianÃ§a
    if (patternAnalysis.intent === llmAnalysis.intent) {
      return {
        intent: patternAnalysis.intent,
        confidence: Math.min(0.95, patternAnalysis.confidence + llmAnalysis.confidence * 0.2),
        entities: { ...patternAnalysis.entities, ...llmAnalysis.entities },
        fallback_used: false
      };
    }

    // Se sÃ£o diferentes, usar a de maior confianÃ§a
    if (llmAnalysis.confidence > patternAnalysis.confidence) {
      return {
        intent: llmAnalysis.intent,
        confidence: llmAnalysis.confidence,
        entities: llmAnalysis.entities,
        fallback_used: false
      };
    } else {
      return {
        intent: patternAnalysis.intent,
        confidence: patternAnalysis.confidence,
        entities: patternAnalysis.entities,
        fallback_used: false
      };
    }
  }
}
```

---

## ğŸ”§ **IMPLEMENTAÃ‡ÃƒO TÃ‰CNICA**

### **Estrutura de Arquivos**
```
conversation-service/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”œâ”€â”€ conversationController.js
â”‚   â”‚   â”œâ”€â”€ messageController.js
â”‚   â”‚   â”œâ”€â”€ memoryController.js
â”‚   â”‚   â”œâ”€â”€ intentController.js
â”‚   â”‚   â””â”€â”€ llmController.js
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ conversationService.js
â”‚   â”‚   â”œâ”€â”€ messageService.js
â”‚   â”‚   â”œâ”€â”€ memoryService.js
â”‚   â”‚   â”œâ”€â”€ intentService.js
â”‚   â”‚   â”œâ”€â”€ llmOrchestrator.js
â”‚   â”‚   â”œâ”€â”€ openaiProvider.js
â”‚   â”‚   â”œâ”€â”€ anthropicProvider.js
â”‚   â”‚   â”œâ”€â”€ googleProvider.js
â”‚   â”‚   â””â”€â”€ cacheService.js
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ conversation.js
â”‚   â”‚   â”œâ”€â”€ message.js
â”‚   â”‚   â”œâ”€â”€ memory.js
â”‚   â”‚   â”œâ”€â”€ intent.js
â”‚   â”‚   â””â”€â”€ llmProvider.js
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ auth.js
â”‚   â”‚   â”œâ”€â”€ validation.js
â”‚   â”‚   â”œâ”€â”€ rateLimit.js
â”‚   â”‚   â””â”€â”€ multiTenant.js
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ conversation.js
â”‚   â”‚   â”œâ”€â”€ message.js
â”‚   â”‚   â”œâ”€â”€ memory.js
â”‚   â”‚   â”œâ”€â”€ intent.js
â”‚   â”‚   â””â”€â”€ llm.js
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.js
â”‚   â”‚   â”œâ”€â”€ validator.js
â”‚   â”‚   â”œâ”€â”€ cache.js
â”‚   â”‚   â”œâ”€â”€ memoryProcessor.js
â”‚   â”‚   â””â”€â”€ intentProcessor.js
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ database.js
â”‚       â”œâ”€â”€ redis.js
â”‚       â”œâ”€â”€ llm.js
â”‚       â””â”€â”€ validation.js
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

### **DependÃªncias Principais**
```json
{
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.3",
    "redis": "^4.6.10",
    "openai": "^4.20.1",
    "@anthropic-ai/sdk": "^0.9.1",
    "@google/generative-ai": "^0.2.1",
    "joi": "^17.11.0",
    "winston": "^3.11.0",
    "helmet": "^7.1.0",
    "cors": "^2.8.5",
    "express-rate-limit": "^7.1.5",
    "compression": "^1.7.4",
    "morgan": "^1.10.0"
  },
  "devDependencies": {
    "jest": "^29.7.0",
    "supertest": "^6.3.3",
    "nodemon": "^3.0.2",
    "eslint": "^8.56.0"
  }
}
```

---

## ğŸ§ª **TESTES IMPLEMENTADOS**

### **Testes UnitÃ¡rios**
- [ ] DetecÃ§Ã£o de intenÃ§Ãµes
- [ ] Sistema de memÃ³ria
- [ ] OrquestraÃ§Ã£o de LLMs
- [ ] Processamento de mensagens

### **Testes de IntegraÃ§Ã£o**
- [ ] IntegraÃ§Ã£o com LLMs
- [ ] Sistema de cache
- [ ] Banco de dados
- [ ] ValidaÃ§Ã£o de endpoints

### **Testes End-to-End**
- [ ] Fluxo completo de conversa
- [ ] Sistema de memÃ³ria
- [ ] Fallbacks de LLM
- [ ] Performance de resposta

---

## ğŸ“Š **MÃ‰TRICAS E MONITORAMENTO**

### **MÃ©tricas de NegÃ³cio**
- **Total de conversas** ativas/encerradas
- **Taxa de resoluÃ§Ã£o** automÃ¡tica
- **Tempo mÃ©dio** de resposta
- **SatisfaÃ§Ã£o** do usuÃ¡rio

### **MÃ©tricas TÃ©cnicas**
- **Response time** dos LLMs
- **Taxa de erro** por provedor
- **Performance** do cache
- **Uso de tokens** e custos

### **Alertas Configurados**
- **LLM down** (todos os provedores)
- **Alta latÃªncia** (> 5 segundos)
- **Alta taxa de erro** (> 10%)
- **Cache miss** (> 30%)

---

## ğŸ”’ **SEGURANÃ‡A IMPLEMENTADA**

### **AutenticaÃ§Ã£o e AutorizaÃ§Ã£o**
- **JWT** com refresh tokens
- **Multi-tenancy** com isolamento completo
- **Rate limiting** por conversa/IP
- **ValidaÃ§Ã£o** de entrada rigorosa

### **ProteÃ§Ã£o de Dados**
- **Criptografia** de dados sensÃ­veis
- **Logs** sem informaÃ§Ãµes pessoais
- **RetenÃ§Ã£o** configurÃ¡vel de dados
- **GDPR compliance** preparado

---

## ğŸš€ **COMO EXECUTAR**

### **1. Configurar Ambiente**
```bash
# Instalar dependÃªncias
npm install

# Configurar variÃ¡veis de ambiente
cp .env.example .env
# Editar .env com chaves de API dos LLMs
```

### **2. Executar Testes**
```bash
# Testes unitÃ¡rios
npm run test:unit

# Testes de integraÃ§Ã£o
npm run test:integration

# Testes end-to-end
npm run test:e2e

# Todos os testes
npm test
```

### **3. Iniciar ServiÃ§o**
```bash
# Desenvolvimento
npm run dev

# ProduÃ§Ã£o
npm start

# Docker
docker-compose up conversation-service
```

---

## ğŸ¯ **CRITÃ‰RIOS DE ACEITAÃ‡ÃƒO**

### **Funcionalidade**
- [ ] Sistema de conversas funcionando
- [ ] MemÃ³ria conversacional operacional
- [ ] OrquestraÃ§Ã£o de LLMs ativa
- [ ] DetecÃ§Ã£o de intenÃ§Ãµes funcionando

### **Performance**
- [ ] Response time < 3 segundos para 95% das respostas
- [ ] Cache hit ratio > 80%
- [ ] Uptime > 99.5%
- [ ] Throughput > 100 conversas simultÃ¢neas

### **Qualidade**
- [ ] Cobertura de testes > 90%
- [ ] DocumentaÃ§Ã£o completa
- [ ] Logs estruturados funcionando
- [ ] MÃ©tricas sendo coletadas

---

## ğŸ† **CONCLUSÃƒO**

O **EntregÃ¡vel 3: Conversation Service** implementa o sistema completo de conversaÃ§Ã£o com memÃ³ria avanÃ§ada, orquestraÃ§Ã£o de LLMs e detecÃ§Ã£o inteligente de intenÃ§Ãµes.

### **Valor Entregue**
- âœ… **Sistema de conversas** multi-tenant completo
- âœ… **MemÃ³ria conversacional** avanÃ§ada e inteligente
- âœ… **OrquestraÃ§Ã£o de LLMs** com fallbacks
- âœ… **DetecÃ§Ã£o de intenÃ§Ãµes** automÃ¡tica
- âœ… **Cache inteligente** para performance
- âœ… **SeguranÃ§a robusta** com isolamento completo

### **Status Final**
**ğŸ”„ ENTREGÃVEL 3 EM DESENVOLVIMENTO**  
**ğŸ“‹ PRONTO PARA IMPLEMENTAÃ‡ÃƒO**

---

**Documento**: specification.md  
**EntregÃ¡vel**: 03-conversation-service  
**Status**: ğŸ”„ EM DESENVOLVIMENTO  
**Data**: 2024-01-15  
**VersÃ£o**: 1.0.0  
**PrÃ³xima Fase**: ImplementaÃ§Ã£o e testes
